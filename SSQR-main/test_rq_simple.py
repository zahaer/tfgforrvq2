#!/usr/bin/env python3
"""
ç®€åŒ–çš„RQ-VAEæµ‹è¯•è„šæœ¬ï¼Œç”¨äºéªŒè¯RQ-VAEæ ¸å¿ƒåŠŸèƒ½
æš‚æ—¶è·³è¿‡dglä¾èµ–ï¼Œç›´æ¥æµ‹è¯•RQCodebook
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os

# æ·»åŠ modelç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'model'))

from RQCodebook import RQCodebook

def test_rq_codebook():
    """æµ‹è¯•RQ-VAEç æœ¬åŠŸèƒ½"""
    print("=== æµ‹è¯•RQ-VAEç æœ¬åŠŸèƒ½ ===")
    
    # è®¾ç½®å‚æ•°
    num_codes = 1024
    latent_dim = 200
    num_quantizers = 4
    batch_size = 32
    seq_len = 16
    
    # åˆ›å»ºRQç æœ¬
    rq_codebook = RQCodebook(
        num_codes=num_codes,
        latent_dim=latent_dim,
        num_quantizers=num_quantizers
    )
    
    print(f"åˆ›å»ºRQç æœ¬: {num_codes}ä¸ªç å‘é‡, {latent_dim}ç»´, {num_quantizers}ä¸ªé‡åŒ–å™¨")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_input = torch.randn(batch_size, seq_len, latent_dim)
    print(f"æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    
    # å‰å‘ä¼ æ’­
    quantized_output, code_indices, loss = rq_codebook(test_input)
    print(f"é‡åŒ–è¾“å‡ºå½¢çŠ¶: {quantized_output.shape}")
    print(f"ä»£ç ç´¢å¼•å½¢çŠ¶: {code_indices.shape}")
    print(f"é‡åŒ–æŸå¤±: {loss.item():.4f}")
    
    # æµ‹è¯•ä»£ç ç”Ÿæˆ
    codes = rq_codebook.cal_codes(test_input)
    print(f"ç”Ÿæˆçš„ä»£ç å½¢çŠ¶: {codes.shape}")
    
    # æµ‹è¯•ä»£ç é‡æ„
    reconstructed = rq_codebook.decode_from_codes(codes)
    print(f"é‡æ„è¾“å‡ºå½¢çŠ¶: {reconstructed.shape}")
    
    # è®¡ç®—é‡æ„è¯¯å·®
    reconstruction_error = torch.mean((test_input - reconstructed) ** 2)
    print(f"é‡æ„è¯¯å·®: {reconstruction_error.item():.4f}")
    
    print("âœ… RQ-VAEç æœ¬æµ‹è¯•å®Œæˆ!")
    return True

def test_rq_training():
    """æµ‹è¯•RQ-VAEè®­ç»ƒè¿‡ç¨‹"""
    print("\n=== æµ‹è¯•RQ-VAEè®­ç»ƒè¿‡ç¨‹ ===")
    
    # è®¾ç½®å‚æ•°
    num_codes = 512
    latent_dim = 128
    num_quantizers = 3
    batch_size = 16
    seq_len = 8
    num_epochs = 5
    
    # åˆ›å»ºRQç æœ¬
    rq_codebook = RQCodebook(
        num_codes=num_codes,
        latent_dim=latent_dim,
        num_quantizers=num_quantizers
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(rq_codebook.parameters(), lr=0.001)
    
    print(f"å¼€å§‹è®­ç»ƒ: {num_epochs}ä¸ªepoch")
    
    for epoch in range(num_epochs):
        # ç”Ÿæˆéšæœºè®­ç»ƒæ•°æ®
        train_data = torch.randn(batch_size, seq_len, latent_dim)
        
        # å‰å‘ä¼ æ’­
        quantized_output, code_indices, loss = rq_codebook(train_data)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}")
    
    print("âœ… RQ-VAEè®­ç»ƒæµ‹è¯•å®Œæˆ!")
    return True

def generate_sample_codes():
    """ç”Ÿæˆç¤ºä¾‹RQä»£ç æ–‡ä»¶"""
    print("\n=== ç”Ÿæˆç¤ºä¾‹RQä»£ç æ–‡ä»¶ ===")
    
    # è®¾ç½®å‚æ•°
    num_entities = 1000
    seq_len = 16
    num_quantizers = 4
    
    # ç”Ÿæˆéšæœºä»£ç 
    sample_codes = torch.randint(0, 1024, (num_entities, seq_len, num_quantizers))
    
    # ä¿å­˜ä»£ç æ–‡ä»¶
    output_file = "codes_new/FB15k-237_16_1024_4_rq.pt"
    torch.save(sample_codes, output_file)
    
    print(f"ç”Ÿæˆç¤ºä¾‹ä»£ç æ–‡ä»¶: {output_file}")
    print(f"ä»£ç å½¢çŠ¶: {sample_codes.shape}")
    print("âœ… ç¤ºä¾‹ä»£ç æ–‡ä»¶ç”Ÿæˆå®Œæˆ!")
    
    return output_file

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹RQ-VAEåŠŸèƒ½æµ‹è¯•")
    
    try:
        # æµ‹è¯•RQç æœ¬åŠŸèƒ½
        test_rq_codebook()
        
        # æµ‹è¯•è®­ç»ƒè¿‡ç¨‹
        test_rq_training()
        
        # ç”Ÿæˆç¤ºä¾‹ä»£ç æ–‡ä»¶
        generate_sample_codes()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("RQ-VAEæ ¸å¿ƒåŠŸèƒ½éªŒè¯æˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    main()
