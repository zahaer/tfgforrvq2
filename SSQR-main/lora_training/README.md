# RQ-VAE LoRAå¾®è°ƒæŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•å°†RQ-VAEç”Ÿæˆçš„tokenè¯­æ–™é€å…¥LLaMA2-7Bè¿›è¡ŒLoRAå¾®è°ƒã€‚

## ğŸ“‹ ç›®å½•

- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
- [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
- [æ¨ç†æµ‹è¯•](#æ¨ç†æµ‹è¯•)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### 1. ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- CUDA 11.0+ (æ¨è)
- 16GB+ GPUå†…å­˜ (LLaMA2-7B)
- 50GB+ ç£ç›˜ç©ºé—´

### 2. å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
cd SSQR-main/lora_training

# è¿è¡Œç¯å¢ƒè®¾ç½®è„šæœ¬
python setup_environment.py

# æˆ–æ‰‹åŠ¨å®‰è£…
pip install -r requirements.txt
```

### 3. è·å–æ¨¡å‹è®¿é—®æƒé™

```bash
# è®¾ç½®Hugging Face Token (ç”¨äºä¸‹è½½LLaMA2)
huggingface-cli login

# è®¾ç½®Wandb Token (ç”¨äºè®­ç»ƒç›‘æ§)
wandb login
```

## ğŸ“Š æ•°æ®å‡†å¤‡

### 1. å‡†å¤‡RQ-VAEä»£ç æ•°æ®

ç¡®ä¿ä½ æœ‰ä»¥ä¸‹æ–‡ä»¶ï¼š
- `codes_new/FB15K-237N_16_1024_4_rq.pt` - RQä»£ç æ–‡ä»¶
- `data/FB15K-237N/entity2text.txt` - å®ä½“æ–‡æœ¬
- `data/FB15K-237N/relation2id.txt` - å…³ç³»æ–‡æœ¬
- `data/FB15K-237N/train2id.txt` - çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„

### 2. ç”Ÿæˆè®­ç»ƒæ•°æ®

```bash
python dataset_generator.py
```

è¿™å°†ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š
- `lora_data/train.json` - è®­ç»ƒæ•°æ®
- `lora_data/validation.json` - éªŒè¯æ•°æ®
- `lora_data/dataset_stats.json` - æ•°æ®ç»Ÿè®¡

### 3. æ•°æ®æ ¼å¼è¯´æ˜

è®­ç»ƒæ•°æ®é‡‡ç”¨Alpacaæ ¼å¼ï¼š

```json
{
  "instruction": "### RQ-VAE Instruction:\nExplain the quantized representation of entity 'Barack Obama':\n<ENT>Barack Obama</ENT> <CODE>Q0_123:Q1_456:Q2_789|Q0_234:Q1_567:Q2_890</CODE>\n\n### Response:\n",
  "input": "",
  "output": "The quantized representation of 'Barack Obama' consists of 4 sequence positions, each with 3 quantizer levels..."
}
```

## ğŸš€ è®­ç»ƒæµç¨‹

### 1. åŸºç¡€è®­ç»ƒ

```bash
python train_lora.py \
    --model_name meta-llama/Llama-2-7b-hf \
    --train_data lora_data/train.json \
    --val_data lora_data/validation.json \
    --output_dir lora_outputs \
    --epochs 3 \
    --batch_size 4 \
    --learning_rate 2e-4 \
    --lora_r 16 \
    --lora_alpha 32
```

### 2. é«˜çº§è®­ç»ƒé…ç½®

```bash
python train_lora.py \
    --model_name meta-llama/Llama-2-7b-hf \
    --train_data lora_data/train.json \
    --val_data lora_data/validation.json \
    --output_dir lora_outputs \
    --epochs 5 \
    --batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 1e-4 \
    --warmup_steps 200 \
    --lora_r 32 \
    --lora_alpha 64 \
    --lora_dropout 0.05 \
    --fp16 \
    --wandb_project rq-lora-experiment
```

### 3. è®­ç»ƒç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ç›‘æ§ï¼š

- **Wandb**: è‡ªåŠ¨è®°å½•è®­ç»ƒæŒ‡æ ‡
- **Tensorboard**: æŸ¥çœ‹è¯¦ç»†è®­ç»ƒæ›²çº¿
- **æ—¥å¿—æ–‡ä»¶**: ä¿å­˜åœ¨`logs/`ç›®å½•

## ğŸ§ª æ¨ç†æµ‹è¯•

### 1. åŸºç¡€æ¨ç†

```bash
python inference.py \
    --base_model meta-llama/Llama-2-7b-hf \
    --lora_adapter lora_outputs/lora_adapters \
    --rq_codes codes_new/FB15K-237N_16_1024_4_rq.pt \
    --entity_texts data/FB15K-237N/entity2text.txt \
    --num_samples 50 \
    --output inference_results.json
```

### 2. è‡ªå®šä¹‰æµ‹è¯•

```python
from inference import RQLoraInference
import numpy as np

# åˆå§‹åŒ–æ¨ç†å™¨
inference = RQLoraInference(
    base_model_name="meta-llama/Llama-2-7b-hf",
    lora_adapter_path="lora_outputs/lora_adapters"
)

# æµ‹è¯•å®ä½“ç†è§£
rq_codes = np.array([[123, 456, 789], [234, 567, 890]])
result = inference.test_entity_understanding(
    entity_name="Barack Obama",
    rq_codes=rq_codes,
    task_type="understanding"
)

print(result["response"])
```

## âš™ï¸ é…ç½®è¯´æ˜

### LoRAå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `lora_r` | 16 | LoRAç§©ï¼Œå½±å“æ¨¡å‹å®¹é‡ |
| `lora_alpha` | 32 | LoRAç¼©æ”¾å‚æ•° |
| `lora_dropout` | 0.1 | LoRA dropoutç‡ |
| `target_modules` | å…¨è¿æ¥å±‚ | åº”ç”¨LoRAçš„æ¨¡å— |

### è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `epochs` | 3 | è®­ç»ƒè½®æ•° |
| `batch_size` | 4 | æ‰¹æ¬¡å¤§å° |
| `learning_rate` | 2e-4 | å­¦ä¹ ç‡ |
| `max_length` | 512 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `gradient_accumulation_steps` | 4 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |

### ç”Ÿæˆå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `temperature` | 0.7 | ç”Ÿæˆæ¸©åº¦ |
| `top_p` | 0.9 | Top-pé‡‡æ · |
| `top_k` | 50 | Top-ké‡‡æ · |
| `repetition_penalty` | 1.1 | é‡å¤æƒ©ç½š |

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. å†…å­˜ä¸è¶³

**é—®é¢˜**: CUDA out of memory

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°‘æ‰¹æ¬¡å¤§å°
--batch_size 1

# å¢åŠ æ¢¯åº¦ç´¯ç§¯
--gradient_accumulation_steps 16

# ä½¿ç”¨8-bitè®­ç»ƒ
pip install bitsandbytes
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ 8-bité…ç½®
```

#### 2. æ¨¡å‹ä¸‹è½½å¤±è´¥

**é—®é¢˜**: æ— æ³•ä¸‹è½½LLaMA2æ¨¡å‹

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è®¾ç½®Hugging Face Token
huggingface-cli login

# æˆ–ä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

#### 3. è®­ç»ƒé€Ÿåº¦æ…¢

**é—®é¢˜**: è®­ç»ƒé€Ÿåº¦è¿‡æ…¢

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨Flash Attention
pip install flash-attn

# å¯ç”¨FP16
--fp16

# ä½¿ç”¨å¤šGPU
accelerate launch train_lora.py [å‚æ•°]
```

#### 4. ç”Ÿæˆè´¨é‡å·®

**é—®é¢˜**: æ¨¡å‹ç”Ÿæˆè´¨é‡ä¸ä½³

**è§£å†³æ–¹æ¡ˆ**:
- å¢åŠ è®­ç»ƒæ•°æ®é‡
- è°ƒæ•´LoRAå‚æ•° (`lora_r`, `lora_alpha`)
- å»¶é•¿è®­ç»ƒæ—¶é—´
- è°ƒæ•´ç”Ÿæˆå‚æ•°

### æ€§èƒ½ä¼˜åŒ–

#### 1. å†…å­˜ä¼˜åŒ–

```python
# ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
training_args.gradient_checkpointing = True

# ä½¿ç”¨DeepSpeed
pip install deepspeed
```

#### 2. é€Ÿåº¦ä¼˜åŒ–

```python
# ä½¿ç”¨ç¼–è¯‘ä¼˜åŒ–
model = torch.compile(model)

# ä½¿ç”¨æ··åˆç²¾åº¦
training_args.fp16 = True
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### 1. è‡ªåŠ¨è¯„ä¼°

- **ä»£ç æå–å‡†ç¡®ç‡**: ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–RQä»£ç çš„å‡†ç¡®ç‡
- **å›°æƒ‘åº¦**: æ¨¡å‹å¯¹æµ‹è¯•æ•°æ®çš„å›°æƒ‘åº¦
- **BLEUåˆ†æ•°**: ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡è¯„ä¼°

### 2. äººå·¥è¯„ä¼°

- **è¯­ä¹‰ä¸€è‡´æ€§**: ç”Ÿæˆå†…å®¹ä¸RQä»£ç çš„è¯­ä¹‰ä¸€è‡´æ€§
- **é€»è¾‘æ­£ç¡®æ€§**: æ¨ç†è¿‡ç¨‹çš„é€»è¾‘æ­£ç¡®æ€§
- **è¯­è¨€æµç•…æ€§**: ç”Ÿæˆæ–‡æœ¬çš„è¯­è¨€æµç•…åº¦

## ğŸ”„ å·¥ä½œæµç¨‹

```mermaid
graph TD
    A[RQ-VAEè®­ç»ƒ] --> B[ç”ŸæˆRQä»£ç ]
    B --> C[åˆ›å»ºè®­ç»ƒæ•°æ®]
    C --> D[LoRAå¾®è°ƒ]
    D --> E[æ¨¡å‹è¯„ä¼°]
    E --> F[æ¨ç†æµ‹è¯•]
    F --> G[éƒ¨ç½²åº”ç”¨]
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [LoRAè®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [PEFTåº“æ–‡æ¡£](https://huggingface.co/docs/peft)
- [LLaMA2æ¨¡å‹](https://huggingface.co/meta-llama/Llama-2-7b-hf)
- [RQ-VAEè®ºæ–‡](https://arxiv.org/abs/2303.01928)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªåŸé¡¹ç›®çš„è®¸å¯è¯ã€‚
