#!/usr/bin/env python3
"""
ç®€åŒ–çš„LoRAæµ‹è¯•è„šæœ¬ï¼Œç”¨äºéªŒè¯RQ tokenå¤„ç†åŠŸèƒ½
æš‚æ—¶è·³è¿‡transformerså’Œpeftä¾èµ–
"""

import sys
import os
import json
import numpy as np
import torch

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(__file__))

def test_rq_token_processor():
    """æµ‹è¯•RQ tokenå¤„ç†å™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    print("=== æµ‹è¯•RQ Tokenå¤„ç†å™¨ ===")
    
    # æ¨¡æ‹ŸRQä»£ç 
    rq_codes = np.array([
        [123, 456, 789, 12],
        [234, 567, 890, 123],
        [345, 678, 901, 234],
        [456, 789, 12, 345]
    ])
    
    print(f"RQä»£ç å½¢çŠ¶: {rq_codes.shape}")
    print(f"RQä»£ç å†…å®¹:\n{rq_codes}")
    
    # æ¨¡æ‹Ÿtokenå¤„ç†
    def rq_codes_to_text(rq_codes, entity_name=""):
        """å°†RQä»£ç è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"""
        seq_len, num_quantizers = rq_codes.shape
        
        # æ„å»ºä»£ç å­—ç¬¦ä¸²
        code_parts = []
        for i in range(seq_len):
            level_codes = []
            for j in range(num_quantizers):
                level_codes.append(f"Q{j}_{rq_codes[i, j]}")
            code_parts.append(":".join(level_codes))
        
        code_str = "|".join(code_parts)
        
        # æ„å»ºå®Œæ•´æ–‡æœ¬
        if entity_name:
            text = f"<ENT>{entity_name}</ENT> <CODE>{code_str}</CODE>"
        else:
            text = f"<CODE>{code_str}</CODE>"
        
        return text
    
    # æµ‹è¯•è½¬æ¢
    entity_name = "Barack Obama"
    text = rq_codes_to_text(rq_codes, entity_name)
    print(f"è½¬æ¢åçš„æ–‡æœ¬: {text}")
    
    # æµ‹è¯•ä»£ç æå–
    def extract_rq_codes_from_text(text):
        """ä»æ–‡æœ¬ä¸­æå–RQä»£ç """
        import re
        
        # åŒ¹é…ä»£ç æ¨¡å¼
        code_pattern = r'<CODE>(.*?)</CODE>'
        match = re.search(code_pattern, text)
        
        if not match:
            return None
        
        code_str = match.group(1)
        
        # è§£æä»£ç 
        try:
            levels = code_str.split("|")
            codes = []
            
            for level in levels:
                quantizers = level.split(":")
                level_codes = []
                
                for quantizer in quantizers:
                    if quantizer.startswith('Q') and '_' in quantizer:
                        code_value = int(quantizer.split('_')[1])
                        level_codes.append(code_value)
                    else:
                        return None
                
                codes.append(level_codes)
            
            return np.array(codes)
        
        except (ValueError, IndexError):
            return None
    
    # æµ‹è¯•æå–
    extracted_codes = extract_rq_codes_from_text(text)
    print(f"æå–çš„ä»£ç å½¢çŠ¶: {extracted_codes.shape}")
    print(f"æå–çš„ä»£ç å†…å®¹:\n{extracted_codes}")
    
    # éªŒè¯ä¸€è‡´æ€§
    if np.array_equal(rq_codes, extracted_codes):
        print("âœ… ä»£ç æå–éªŒè¯æˆåŠŸ!")
    else:
        print("âŒ ä»£ç æå–éªŒè¯å¤±è´¥!")
    
    return True

def test_dataset_generation():
    """æµ‹è¯•æ•°æ®é›†ç”Ÿæˆï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    print("\n=== æµ‹è¯•æ•°æ®é›†ç”Ÿæˆ ===")
    
    # æ¨¡æ‹Ÿå®ä½“æ•°æ®
    entities = ["Barack Obama", "Donald Trump", "Joe Biden", "Hillary Clinton"]
    relations = ["president_of", "spouse_of", "born_in", "educated_at"]
    
    # æ¨¡æ‹ŸRQä»£ç 
    num_entities = len(entities)
    seq_len = 4
    num_quantizers = 3
    
    rq_codes = np.random.randint(0, 1024, (num_entities, seq_len, num_quantizers))
    
    print(f"å®ä½“æ•°é‡: {num_entities}")
    print(f"å…³ç³»æ•°é‡: {len(relations)}")
    print(f"RQä»£ç å½¢çŠ¶: {rq_codes.shape}")
    
    # ç”Ÿæˆè®­ç»ƒä»»åŠ¡
    tasks = []
    
    for i, entity in enumerate(entities):
        entity_codes = rq_codes[i]
        
        # ç”Ÿæˆå®ä½“è¡¨ç¤ºä»»åŠ¡
        task = {
            "instruction": f"Explain the quantized representation of entity '{entity}':\n<ENT>{entity}</ENT> <CODE>{':'.join([f'Q{j}_{entity_codes[0, j]}' for j in range(num_quantizers)])}</CODE>",
            "input": "",
            "output": f"The quantized representation of '{entity}' consists of {seq_len} sequence positions, each with {num_quantizers} quantizer levels.",
            "task_type": "understanding",
            "entity_id": i
        }
        tasks.append(task)
    
    print(f"ç”Ÿæˆäº† {len(tasks)} ä¸ªè®­ç»ƒä»»åŠ¡")
    
    # ä¿å­˜ä»»åŠ¡æ•°æ®
    output_file = "lora_data/test_tasks.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(tasks, f, ensure_ascii=False, indent=2)
    
    print(f"ä»»åŠ¡æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    # éªŒè¯ä¿å­˜çš„æ•°æ®
    with open(output_file, 'r', encoding='utf-8') as f:
        loaded_tasks = json.load(f)
    
    print(f"éªŒè¯: åŠ è½½äº† {len(loaded_tasks)} ä¸ªä»»åŠ¡")
    print("âœ… æ•°æ®é›†ç”Ÿæˆæµ‹è¯•å®Œæˆ!")
    
    return True

def test_training_data_format():
    """æµ‹è¯•è®­ç»ƒæ•°æ®æ ¼å¼"""
    print("\n=== æµ‹è¯•è®­ç»ƒæ•°æ®æ ¼å¼ ===")
    
    # åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®
    sample_data = {
        "instruction": "### RQ-VAE Instruction:\nExplain the quantized representation of entity 'Barack Obama':\n<ENT>Barack Obama</ENT> <CODE>Q0_123:Q1_456:Q2_789|Q0_234:Q1_567:Q2_890</CODE>\n\n### Response:\n",
        "input": "",
        "output": "The quantized representation of 'Barack Obama' consists of 4 sequence positions, each with 3 quantizer levels. This representation captures the entity's semantic properties in a compressed format."
    }
    
    print("ç¤ºä¾‹è®­ç»ƒæ•°æ®:")
    print(json.dumps(sample_data, ensure_ascii=False, indent=2))
    
    # éªŒè¯æ•°æ®æ ¼å¼
    required_fields = ["instruction", "input", "output"]
    for field in required_fields:
        if field in sample_data:
            print(f"âœ… å­—æ®µ '{field}' å­˜åœ¨")
        else:
            print(f"âŒ å­—æ®µ '{field}' ç¼ºå¤±")
    
    print("âœ… è®­ç»ƒæ•°æ®æ ¼å¼æµ‹è¯•å®Œæˆ!")
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹LoRAåŠŸèƒ½æµ‹è¯•")
    
    try:
        # æµ‹è¯•RQ tokenå¤„ç†å™¨
        test_rq_token_processor()
        
        # æµ‹è¯•æ•°æ®é›†ç”Ÿæˆ
        test_dataset_generation()
        
        # æµ‹è¯•è®­ç»ƒæ•°æ®æ ¼å¼
        test_training_data_format()
        
        print("\nğŸ‰ æ‰€æœ‰LoRAæµ‹è¯•å®Œæˆ!")
        print("LoRAæ ¸å¿ƒåŠŸèƒ½éªŒè¯æˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    main()
