#!/usr/bin/env python3
"""
å®Œæ•´çš„RQ-VAEé¡¹ç›®æ¼”ç¤ºè„šæœ¬
å±•ç¤ºä»RQ-VAEè®­ç»ƒåˆ°LoRAå¾®è°ƒçš„å®Œæ•´æµç¨‹
"""

import os
import sys
import torch
import numpy as np
import json
from pathlib import Path

# æ·»åŠ modelç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'model'))

def demo_rq_vae_training():
    """æ¼”ç¤ºRQ-VAEè®­ç»ƒè¿‡ç¨‹"""
    print("ğŸ¯ æ­¥éª¤1: RQ-VAEè®­ç»ƒæ¼”ç¤º")
    print("=" * 50)
    
    from RQCodebook import RQCodebook
    
    # è®¾ç½®å‚æ•°
    num_codes = 1024
    latent_dim = 200
    num_quantizers = 4
    batch_size = 64
    seq_len = 16
    num_epochs = 10
    
    print(f"è®­ç»ƒå‚æ•°:")
    print(f"  - ç æœ¬å¤§å°: {num_codes}")
    print(f"  - æ½œåœ¨ç»´åº¦: {latent_dim}")
    print(f"  - é‡åŒ–å™¨æ•°é‡: {num_quantizers}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  - åºåˆ—é•¿åº¦: {seq_len}")
    print(f"  - è®­ç»ƒè½®æ•°: {num_epochs}")
    
    # åˆ›å»ºRQç æœ¬
    rq_codebook = RQCodebook(
        num_codes=num_codes,
        latent_dim=latent_dim,
        num_quantizers=num_quantizers
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(rq_codebook.parameters(), lr=0.001)
    
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        # ç”Ÿæˆéšæœºè®­ç»ƒæ•°æ®ï¼ˆæ¨¡æ‹ŸçŸ¥è¯†å›¾è°±å®ä½“åµŒå…¥ï¼‰
        train_data = torch.randn(batch_size, seq_len, latent_dim)
        
        # å‰å‘ä¼ æ’­
        quantized_output, code_indices, loss = rq_codebook(train_data)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 2 == 0:
            print(f"  Epoch {epoch+1:2d}/{num_epochs}: Loss = {loss.item():.4f}")
    
    print(f"âœ… RQ-VAEè®­ç»ƒå®Œæˆ!")
    
    # ç”Ÿæˆå®ä½“ä»£ç 
    print(f"\nç”Ÿæˆå®ä½“ä»£ç ...")
    num_entities = 1000
    entity_embeddings = torch.randn(num_entities, seq_len, latent_dim)
    
    with torch.no_grad():
        entity_codes = rq_codebook.cal_codes(entity_embeddings)
    
    # ä¿å­˜ä»£ç æ–‡ä»¶
    codes_dir = Path("codes_new")
    codes_dir.mkdir(exist_ok=True)
    
    codes_file = codes_dir / "FB15k-237_16_1024_4_rq.pt"
    torch.save(entity_codes, codes_file)
    
    print(f"âœ… å®ä½“ä»£ç å·²ä¿å­˜åˆ°: {codes_file}")
    print(f"   ä»£ç å½¢çŠ¶: {entity_codes.shape}")
    
    return codes_file, rq_codebook

def demo_data_generation(codes_file):
    """æ¼”ç¤ºæ•°æ®ç”Ÿæˆè¿‡ç¨‹"""
    print("\nğŸ¯ æ­¥éª¤2: LoRAè®­ç»ƒæ•°æ®ç”Ÿæˆæ¼”ç¤º")
    print("=" * 50)
    
    # åŠ è½½RQä»£ç 
    entity_codes = torch.load(codes_file)
    num_entities, seq_len, num_quantizers = entity_codes.shape
    
    print(f"åŠ è½½RQä»£ç :")
    print(f"  - å®ä½“æ•°é‡: {num_entities}")
    print(f"  - åºåˆ—é•¿åº¦: {seq_len}")
    print(f"  - é‡åŒ–å™¨æ•°é‡: {num_quantizers}")
    
    # æ¨¡æ‹Ÿå®ä½“åç§°
    entity_names = [f"Entity_{i:04d}" for i in range(num_entities)]
    
    # ç”Ÿæˆè®­ç»ƒä»»åŠ¡
    tasks = []
    
    # å®ä½“è¡¨ç¤ºå­¦ä¹ ä»»åŠ¡
    for i in range(min(100, num_entities)):  # ç”Ÿæˆ100ä¸ªä»»åŠ¡
        entity_name = entity_names[i]
        entity_code = entity_codes[i]
        
        # ç”Ÿæˆä»£ç å­—ç¬¦ä¸²
        code_parts = []
        for pos in range(seq_len):
            level_codes = [f"Q{j}_{entity_code[pos, j].item()}" for j in range(num_quantizers)]
            code_parts.append(":".join(level_codes))
        code_str = "|".join(code_parts)
        
        # åˆ›å»ºä»»åŠ¡
        task = {
            "instruction": f"Explain the quantized representation of entity '{entity_name}':\n<ENT>{entity_name}</ENT> <CODE>{code_str}</CODE>",
            "input": "",
            "output": f"The quantized representation of '{entity_name}' consists of {seq_len} sequence positions, each with {num_quantizers} quantizer levels. This representation captures the entity's semantic properties in a compressed format.",
            "task_type": "understanding",
            "entity_id": i
        }
        tasks.append(task)
    
    # çŸ¥è¯†æ¨ç†ä»»åŠ¡
    for i in range(min(50, num_entities // 2)):
        head_entity = entity_names[i * 2]
        tail_entity = entity_names[i * 2 + 1]
        relation = f"relation_{i % 10}"
        
        head_code = entity_codes[i * 2]
        tail_code = entity_codes[i * 2 + 1]
        
        # ç”Ÿæˆä»£ç å­—ç¬¦ä¸²
        def code_to_str(code):
            code_parts = []
            for pos in range(seq_len):
                level_codes = [f"Q{j}_{code[pos, j].item()}" for j in range(num_quantizers)]
                code_parts.append(":".join(level_codes))
            return "|".join(code_parts)
        
        head_code_str = code_to_str(head_code)
        tail_code_str = code_to_str(tail_code)
        
        task = {
            "instruction": f"Validate the knowledge triplet:\n<ENT>{head_entity}</ENT> <CODE>{head_code_str}</CODE>\nRelation: {relation}\n<ENT>{tail_entity}</ENT> <CODE>{tail_code_str}</CODE>\nIs this triplet valid?",
            "input": "",
            "output": f"Yes, this knowledge triplet is valid. The relationship between '{head_entity}' and '{tail_entity}' through '{relation}' is supported by their quantized representations.",
            "task_type": "validation",
            "triplet": (i * 2, i % 10, i * 2 + 1)
        }
        tasks.append(task)
    
    print(f"ç”Ÿæˆäº† {len(tasks)} ä¸ªè®­ç»ƒä»»åŠ¡")
    print(f"  - å®ä½“è¡¨ç¤ºä»»åŠ¡: 100")
    print(f"  - çŸ¥è¯†æ¨ç†ä»»åŠ¡: 50")
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    lora_data_dir = Path("lora_training/lora_data")
    lora_data_dir.mkdir(exist_ok=True)
    
    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    train_tasks = tasks[:120]
    val_tasks = tasks[120:]
    
    train_file = lora_data_dir / "train.json"
    val_file = lora_data_dir / "validation.json"
    
    with open(train_file, 'w', encoding='utf-8') as f:
        json.dump(train_tasks, f, ensure_ascii=False, indent=2)
    
    with open(val_file, 'w', encoding='utf-8') as f:
        json.dump(val_tasks, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜:")
    print(f"  - è®­ç»ƒé›†: {train_file} ({len(train_tasks)} ä¸ªä»»åŠ¡)")
    print(f"  - éªŒè¯é›†: {val_file} ({len(val_tasks)} ä¸ªä»»åŠ¡)")
    
    return train_file, val_file

def demo_lora_training(train_file, val_file):
    """æ¼”ç¤ºLoRAè®­ç»ƒè¿‡ç¨‹ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    print("\nğŸ¯ æ­¥éª¤3: LoRAå¾®è°ƒæ¼”ç¤º")
    print("=" * 50)
    
    print("LoRAè®­ç»ƒé…ç½®:")
    print("  - åŸºç¡€æ¨¡å‹: LLaMA2-7B")
    print("  - LoRAç§©: 16")
    print("  - LoRA Alpha: 32")
    print("  - å­¦ä¹ ç‡: 2e-4")
    print("  - æ‰¹æ¬¡å¤§å°: 4")
    print("  - è®­ç»ƒè½®æ•°: 3")
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    print(f"\næ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹...")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    with open(train_file, 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    
    with open(val_file, 'r', encoding='utf-8') as f:
        val_data = json.load(f)
    
    print(f"  - è®­ç»ƒæ ·æœ¬: {len(train_data)}")
    print(f"  - éªŒè¯æ ·æœ¬: {len(val_data)}")
    
    # æ¨¡æ‹Ÿè®­ç»ƒæŒ‡æ ‡
    print(f"\næ¨¡æ‹Ÿè®­ç»ƒæŒ‡æ ‡:")
    for epoch in range(3):
        train_loss = 2.5 - epoch * 0.3 + np.random.normal(0, 0.1)
        val_loss = 2.8 - epoch * 0.2 + np.random.normal(0, 0.1)
        print(f"  Epoch {epoch+1}: Train Loss = {train_loss:.3f}, Val Loss = {val_loss:.3f}")
    
    print(f"âœ… LoRAè®­ç»ƒå®Œæˆ!")
    
    # ä¿å­˜æ¨¡å‹
    model_dir = Path("lora_training/lora_outputs")
    model_dir.mkdir(exist_ok=True)
    
    adapter_dir = model_dir / "lora_adapters"
    adapter_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„æ¨¡å‹é…ç½®æ–‡ä»¶
    config = {
        "base_model_name": "meta-llama/Llama-2-7b-hf",
        "lora_config": {
            "r": 16,
            "lora_alpha": 32,
            "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
            "lora_dropout": 0.1
        },
        "training_args": {
            "learning_rate": 2e-4,
            "batch_size": 4,
            "num_epochs": 3
        }
    }
    
    config_file = adapter_dir / "adapter_config.json"
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… LoRAé€‚é…å™¨å·²ä¿å­˜åˆ°: {adapter_dir}")
    
    return adapter_dir

def demo_inference(adapter_dir, codes_file):
    """æ¼”ç¤ºæ¨ç†è¿‡ç¨‹"""
    print("\nğŸ¯ æ­¥éª¤4: æ¨¡å‹æ¨ç†æ¼”ç¤º")
    print("=" * 50)
    
    # åŠ è½½RQä»£ç 
    entity_codes = torch.load(codes_file)
    
    print("æ¨ç†æµ‹è¯•:")
    print("  - åŠ è½½LoRAé€‚é…å™¨")
    print("  - åŠ è½½RQä»£ç ")
    print("  - æ‰§è¡Œæ¨ç†")
    
    # æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
    test_entities = ["Entity_0001", "Entity_0002", "Entity_0003"]
    
    for i, entity_name in enumerate(test_entities):
        entity_code = entity_codes[i]
        
        # ç”Ÿæˆä»£ç å­—ç¬¦ä¸²
        code_parts = []
        for pos in range(entity_code.shape[0]):
            level_codes = [f"Q{j}_{entity_code[pos, j].item()}" for j in range(entity_code.shape[1])]
            code_parts.append(":".join(level_codes))
        code_str = "|".join(code_parts)
        
        # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡º
        prompt = f"Explain the quantized representation of entity '{entity_name}':\n<ENT>{entity_name}</ENT> <CODE>{code_str}</CODE>"
        
        # æ¨¡æ‹Ÿç”Ÿæˆå›ç­”
        response = f"The quantized representation of '{entity_name}' consists of {entity_code.shape[0]} sequence positions, each with {entity_code.shape[1]} quantizer levels. This representation captures the entity's semantic properties in a compressed format, enabling efficient knowledge graph reasoning."
        
        print(f"\næµ‹è¯• {i+1}: {entity_name}")
        print(f"  è¾“å…¥: {prompt[:100]}...")
        print(f"  è¾“å‡º: {response[:100]}...")
    
    print(f"\nâœ… æ¨ç†æµ‹è¯•å®Œæˆ!")
    
    # ä¿å­˜æ¨ç†ç»“æœ
    results = {
        "test_entities": test_entities,
        "inference_results": [
            {
                "entity": entity,
                "prompt": f"Explain the quantized representation of entity '{entity}'",
                "response": f"The quantized representation of '{entity}' consists of multiple sequence positions with quantizer levels."
            }
            for entity in test_entities
        ]
    }
    
    results_file = Path("lora_training/inference_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ¨ç†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ RQ-VAEé¡¹ç›®å®Œæ•´æ¼”ç¤º")
    print("=" * 60)
    print("æœ¬æ¼”ç¤ºå°†å±•ç¤ºä»RQ-VAEè®­ç»ƒåˆ°LoRAå¾®è°ƒçš„å®Œæ•´æµç¨‹")
    print("=" * 60)
    
    try:
        # æ­¥éª¤1: RQ-VAEè®­ç»ƒ
        codes_file, rq_codebook = demo_rq_vae_training()
        
        # æ­¥éª¤2: æ•°æ®ç”Ÿæˆ
        train_file, val_file = demo_data_generation(codes_file)
        
        # æ­¥éª¤3: LoRAè®­ç»ƒ
        adapter_dir = demo_lora_training(train_file, val_file)
        
        # æ­¥éª¤4: æ¨ç†æµ‹è¯•
        demo_inference(adapter_dir, codes_file)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ å®Œæ•´æ¼”ç¤ºæˆåŠŸå®Œæˆ!")
        print("=" * 60)
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  - RQä»£ç : {codes_file}")
        print(f"  - è®­ç»ƒæ•°æ®: {train_file}")
        print(f"  - éªŒè¯æ•°æ®: {val_file}")
        print(f"  - LoRAé€‚é…å™¨: {adapter_dir}")
        print(f"  - æ¨ç†ç»“æœ: lora_training/inference_results.json")
        print("\né¡¹ç›®å·²æˆåŠŸé…ç½®å¹¶è¿è¡Œ!")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    main()
